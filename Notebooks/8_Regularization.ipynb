{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "132cedb6",
   "metadata": {},
   "source": [
    "# Regularization techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b0961e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import torch\n",
    "from torch import nn\n",
    "from d2l import torch as d2l"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "414ecd20",
   "metadata": {},
   "source": [
    "# Regularization in statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a822eeef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# -----------------------------\n",
    "# 1) Data generation (10-D)\n",
    "# -----------------------------\n",
    "np.random.seed(42)\n",
    "\n",
    "n_train = 30\n",
    "n_test = 200\n",
    "d = 10\n",
    "\n",
    "# Ground-truth weights: sparse-ish with mixed signs\n",
    "true_w = np.array([2.5, -1.7, 0.0, 0.0, 1.2, 0.0, 0.0, -0.5, 0.0, 0.8])\n",
    "true_b = 0.5\n",
    "noise_sigma = 1.0\n",
    "\n",
    "X_train = np.random.randn(n_train, d)\n",
    "X_test  = np.random.randn(n_test, d)\n",
    "\n",
    "y_train = X_train @ true_w + true_b + np.random.normal(scale=noise_sigma, size=n_train)\n",
    "y_test  = X_test  @ true_w + true_b + np.random.normal(scale=noise_sigma, size=n_test)\n",
    "\n",
    "# Add intercept column as LAST column (will NOT be regularized)\n",
    "Xtr = np.hstack([X_train, np.ones((n_train, 1))])\n",
    "Xte = np.hstack([X_test,  np.ones((n_test, 1))])\n",
    "\n",
    "# -----------------------------\n",
    "# 2) Ridge closed-form (no reg on intercept)\n",
    "# -----------------------------\n",
    "def ridge_regression(X, y, alpha):\n",
    "    \"\"\"Return w including intercept as the last element.\n",
    "    Intercept column is assumed to be the last column of X and is NOT regularized.\n",
    "    \"\"\"\n",
    "    p = X.shape[1]\n",
    "    I = np.eye(p)\n",
    "    I[-1, -1] = 0.0  # do not regularize the intercept\n",
    "    return np.linalg.inv(X.T @ X + alpha * I) @ (X.T @ y)\n",
    "\n",
    "def mse(y_true, y_pred):\n",
    "    return float(np.mean((y_true - y_pred) ** 2))\n",
    "\n",
    "# -----------------------------\n",
    "# 3) Fit for α=0 and a grid of α>0\n",
    "# -----------------------------\n",
    "alphas_grid = np.logspace(-3, 3, 60)   # α > 0 for smooth log plot\n",
    "alpha0 = 0.0\n",
    "\n",
    "# α = 0 (ordinary least squares)\n",
    "w0 = ridge_regression(Xtr, y_train, alpha0)\n",
    "ytr0 = Xtr @ w0\n",
    "yte0 = Xte @ w0\n",
    "mse_tr_0 = mse(y_train, ytr0)\n",
    "mse_te_0 = mse(y_test,  yte0)\n",
    "\n",
    "# α > 0\n",
    "weights_grid = []\n",
    "mse_tr_grid = []\n",
    "mse_te_grid = []\n",
    "for a in alphas_grid:\n",
    "    w = ridge_regression(Xtr, y_train, a)\n",
    "    weights_grid.append(w)\n",
    "    mse_tr_grid.append(mse(y_train, Xtr @ w))\n",
    "    mse_te_grid.append(mse(y_test,  Xte @ w))\n",
    "\n",
    "weights_grid = np.asarray(weights_grid)\n",
    "mse_tr_grid = np.asarray(mse_tr_grid)\n",
    "mse_te_grid = np.asarray(mse_te_grid)\n",
    "\n",
    "# Best alpha on test (for demo; in practice use val/CV)\n",
    "best_idx = int(np.argmin(mse_te_grid))\n",
    "best_alpha = float(alphas_grid[best_idx])\n",
    "best_test_mse = float(mse_te_grid[best_idx])\n",
    "\n",
    "# Also keep a few representative alphas for the weight plot\n",
    "alphas_show = [0, 1, 10, 50]\n",
    "weights_show = []\n",
    "for a in alphas_show:\n",
    "    w = ridge_regression(Xtr, y_train, a)\n",
    "    weights_show.append(w)\n",
    "weights_show = np.asarray(weights_show)\n",
    "\n",
    "# -----------------------------\n",
    "# 4) Plots\n",
    "# -----------------------------\n",
    "plt.rcParams['axes.prop_cycle'] = plt.cycler(color=plt.cm.tab20.colors)\n",
    "fig = plt.figure(figsize=(12, 5))\n",
    "\n",
    "# (A) Weight trajectories for selected alphas\n",
    "ax1 = fig.add_subplot(1, 2, 1)\n",
    "dims = np.arange(1, d + 1)\n",
    "for w, a in zip(weights_show, alphas_show):\n",
    "    ax1.plot(dims, abs(w[:-1]), marker='o', label=f\"α={a}\")\n",
    "ax1.plot(dims, abs(true_w), linestyle='--', linewidth=3, label=\"True weights\", color='black')\n",
    "ax1.axhline(0, lw=0.7, color='gray')\n",
    "ax1.set_xticks(dims)\n",
    "ax1.set_xlabel(\"Feature index (1–10)\")\n",
    "ax1.set_ylabel(\"|Weight value|\")\n",
    "ax1.set_title(\"Weight estimates vs. feature index\")\n",
    "ax1.legend()\n",
    "\n",
    "# (B) MSE vs α (log x), with α=0 shown as horizontal dotted lines\n",
    "ax2 = fig.add_subplot(1, 2, 2)\n",
    "ax2.semilogx(alphas_grid, mse_tr_grid, marker='o', label=\"Train MSE\")\n",
    "ax2.semilogx(alphas_grid, mse_te_grid, marker='o', label=\"Test MSE\")\n",
    "# show α=0 results as horizontal lines\n",
    "ax2.axhline(mse_tr_0, linestyle=':', linewidth=1.5, label=f\"Train MSE (α=0) = {mse_tr_0:.3f}\")\n",
    "ax2.axhline(mse_te_0, linestyle='--', linewidth=1.5, label=f\"Test MSE (α=0) = {mse_te_0:.3f}\")\n",
    "# annotate best alpha on grid\n",
    "ax2.axvline(best_alpha, color='k', lw=1, alpha=0.4)\n",
    "ax2.annotate(f\"best α≈{best_alpha:.3g}\\nTest MSE={best_test_mse:.3f}\",\n",
    "             xy=(best_alpha, best_test_mse),\n",
    "             xytext=(1.5*best_alpha, best_test_mse+0.5),\n",
    "             arrowprops=dict(arrowstyle=\"->\", lw=1))\n",
    "ax2.set_xlabel(\"α (log scale)\")\n",
    "ax2.set_ylabel(\"Mean Squared Error\")\n",
    "ax2.set_title(\"Train/Test MSE vs regularization strength\")\n",
    "ax2.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# -----------------------------\n",
    "# 5) Print compact table\n",
    "# -----------------------------\n",
    "print(\"Selected α values (including α=0):\")\n",
    "print(\"Alpha | Intercept | ||w||₂ (no intercept) | Train MSE | Test MSE\")\n",
    "print(\"---------------------------------------------------------------\")\n",
    "for a, w in zip(alphas_show, weights_show):\n",
    "    l2 = np.linalg.norm(w[:-1])\n",
    "    ytr = Xtr @ w\n",
    "    yte = Xte @ w\n",
    "    print(f\"{a:<5} | {w[-1]:9.3f} | {l2:21.3f} | {mse(y_train,ytr):9.3f} | {mse(y_test,yte):8.3f}\")\n",
    "\n",
    "print(\"\\nα=0 (no reg.) baseline:\")\n",
    "print(f\"  Train MSE = {mse_tr_0:.3f}  |  Test MSE = {mse_te_0:.3f}\")\n",
    "print(f\"Best α on grid ≈ {best_alpha:.4g}  with Test MSE = {best_test_mse:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c998dd",
   "metadata": {},
   "source": [
    "## Weight Decay"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c11d76b0",
   "metadata": {},
   "source": [
    "We can illustrate the benefits of weight decay through a simple synthetic example.\n",
    "\n",
    "\n",
    "In this synthetic dataset, our label is given by an underlying linear function of our inputs, corrupted by Gaussian noise with zero mean and standard deviation 0.01. For illustrative purposes, we can make the effects of overfitting pronounced, by increasing the dimentionality to d=200, and working with a small training set with only 20 examples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "396e7115",
   "metadata": {},
   "source": [
    "### Implementation from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ce3ebd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data(d2l.DataModule):\n",
    "\n",
    "    def __init__(self, num_train, num_val, num_features, batch_size):\n",
    "        self.save_hyperparameters()\n",
    "        n = num_train + num_val\n",
    "        self.X = torch.randn(n, num_features)\n",
    "        noise = torch.randn(n, 1) * 0.01\n",
    "        w, b = torch.ones((num_features, 1)) * 0.01, 0.05\n",
    "        self.y = torch.matmul(self.X, w) + b + noise\n",
    "        \n",
    "    def get_dataloader(self, train):\n",
    "        i = slice(0, self.num_train) if train else slice(self.num_train, None)\n",
    "        return self.get_tensorloader([self.X, self.y], train, i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d57ec98d",
   "metadata": {},
   "source": [
    "Define the penalty term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ccecdf51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def l2_penalty(w):\n",
    "    return (w ** 2).sum() / 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d6a2afc",
   "metadata": {},
   "source": [
    "and a weight decay routine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a6118208",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightDecayScratch(d2l.LinearRegressionScratch):\n",
    "\n",
    "    def __init__(self, num_features, lambd, lr, sigma=0.01):\n",
    "        super().__init__(num_features, lr, sigma)\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "    def loss(self, y_hat, y):\n",
    "        return (super().loss(y_hat, y) +\n",
    "                self.lambd * l2_penalty(self.w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc28670",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = Data(num_train=20, num_val=100, num_features=200, batch_size=5)\n",
    "trainer = d2l.Trainer(max_epochs=10)\n",
    "\n",
    "def train_scratch(lambd):\n",
    "    model = WeightDecayScratch(num_features=200, lambd=lambd, lr=0.01)\n",
    "    model.board.yscale='log' # Log scale for better visibility\n",
    "    trainer.fit(model, data)\n",
    "    print('L2 norm of w:', float(l2_penalty(model.w)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c21ff0f4",
   "metadata": {},
   "source": [
    "### Training without Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b907cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_scratch(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "619a9ab5",
   "metadata": {},
   "source": [
    "### Train with Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ec49d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_scratch(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a7c2bc",
   "metadata": {},
   "source": [
    "### Concise implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed0d9d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightDecay(d2l.LinearRegression):\n",
    "    \n",
    "    def __init__(self, wd, lr):\n",
    "        super().__init__(lr)\n",
    "        self.save_hyperparameters()\n",
    "        self.wd = wd\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # note: both LR and weight decay are set here in th eoptimizer!\n",
    "        return torch.optim.SGD([ \n",
    "            {'params': self.net.weight, 'weight_decay': self.wd},\n",
    "            {'params': self.net.bias}], lr=self.lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "258cf9f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = Data(num_train=20, num_val=100, num_features=200, batch_size=5)\n",
    "trainer = d2l.Trainer(max_epochs=20)\n",
    "\n",
    "model = WeightDecay(wd=5, lr=0.001)\n",
    "model.board.yscale='log'\n",
    "trainer.fit(model, data)\n",
    "print('L2 norm of w:', float(l2_penalty(model.get_w_b()[0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f872ab",
   "metadata": {},
   "source": [
    "## Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60e618b8",
   "metadata": {},
   "source": [
    "### Ex.1 Try to improve this learning problem by:\n",
    "- Changing LR \n",
    "- Changing $\\lambda$\n",
    "- Increasing the number of epochs\n",
    "- Simulating more data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de50c7dd",
   "metadata": {},
   "source": [
    "### Ex. 2 Try to use an L1 regularization\n",
    "\n",
    "What would the update equations look like if instead of $∥w∥^2$ we used $\\sum_𝑖|𝑤_𝑖|$ as our penalty of choice (l1 regularization)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32fbc22a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
