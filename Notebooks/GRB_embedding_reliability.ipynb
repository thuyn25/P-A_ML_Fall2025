{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0ac770d",
   "metadata": {},
   "source": [
    "\n",
    "# Reliability of 2D Embeddings with UMAP and scDEED-inspired Method\n",
    "\n",
    "In this lesson, we will explore how to evaluate the reliability of 2D embeddings obtained with UMAP, \n",
    "using an approach inspired by the **scDEED** algorithm.  \n",
    "We will apply it to the **Gamma-Ray Burst (GRB) latent-space datasets**. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f29938",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from scipy.stats import pearsonr\n",
    "import umap.umap_ as umap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e22bf606",
   "metadata": {},
   "source": [
    "\n",
    "## Step 1: Load the GRB Latent-Space Dataset\n",
    "\n",
    "We use pre-computed GRB latent-space vectors (30-dimensional). These are the products of three Convolutional Autoencoders, each one applied to a specific Waterfall plot dimension.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd2256eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load GRB latent space datasets (provided separately as .npy files)\n",
    "hl_1 = np.load('scdeed_data/short_timescales.npy')\n",
    "hl_2 = np.load('scdeed_data/medium_timescales.npy')\n",
    "hl_3 = np.load('scdeed_data/long_timescales.npy')\n",
    "\n",
    "# Normalize each part\n",
    "def normalize(x):\n",
    "    return (x - np.min(x)) / (np.max(x) - np.min(x))\n",
    "\n",
    "hl_1, hl_2, hl_3 = normalize(hl_1), normalize(hl_2), normalize(hl_3)\n",
    "\n",
    "# Concatenate into final latent space\n",
    "dataset = np.concatenate((hl_1, hl_2, hl_3), axis=1)\n",
    "\n",
    "print(\"Number of GRBs:\", dataset.shape[0])\n",
    "print(\"Dimensionality before UMAP:\", dataset.shape[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae087dd1",
   "metadata": {},
   "source": [
    "\n",
    "## Step 2: Apply UMAP for Dimensionality Reduction\n",
    "\n",
    "We now reduce the latent space (30D) into **2D** using UMAP.  \n",
    "This makes the data easier to visualize and explore.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c74fbd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# UMAP parameters\n",
    "NN = 30\n",
    "MD = 0.0\n",
    "LR = 0.001\n",
    "NEPOCHS = 1000\n",
    "LOC_CON = 0.5\n",
    "\n",
    "reducer = umap.UMAP(n_neighbors=NN, min_dist=MD, n_components=2,\n",
    "                    metric='euclidean', n_epochs=NEPOCHS,\n",
    "                    local_connectivity=LOC_CON, learning_rate=LR,\n",
    "                    random_state=42)\n",
    "X_latent = reducer.fit_transform(dataset)\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.scatter(X_latent[:,0], X_latent[:,1], s=10, alpha=0.7)\n",
    "plt.title(\"UMAP 2D Embedding of GRB Latent Space\")\n",
    "plt.xlabel(\"UMAP-1\")\n",
    "plt.ylabel(\"UMAP-2\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28ab08a0",
   "metadata": {},
   "source": [
    "# How can we assess the reliability of the position of each event in the final distribution?\n",
    "\n",
    "UMAP is a powerful algorithm which reduces the dimensionality of a dataset exploiting complex and non-linear relations between events. When used to generally look at a low dimensional distribution, without caring for the position of specific events, the lack of interpretability is usually not an issue. In our case, where we want to investigate the properties and the location of specific GRBs, this could represent an obstacle. \n",
    "Let's try to understand how we can improve the reliability of the final embedding."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c2bb5c",
   "metadata": {},
   "source": [
    "\n",
    "## Step 3: Reliability Assessment step by step\n",
    "\n",
    "We now proceed to:  \n",
    "- Compute reliability scores (Pearson correlation of neighbor distances).  \n",
    "- Generate null distributions by permuting the dataset.  \n",
    "- Visualize real vs null distributions.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bb714cd",
   "metadata": {},
   "source": [
    "Firstly, we begin by evaluating the correlation of the distances of the closest k events in the original and embedded space, calculated in the embedded space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f23549b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We define the \"neighborhood\" of an event.\n",
    "n_samples = dataset.shape[0]\n",
    "k = int(n_samples/2)\n",
    "\n",
    "# We evaluate the distance between events in the original space and in the embedded space.\n",
    "dist_orig = pairwise_distances(dataset)\n",
    "dist_latent = pairwise_distances(X_latent)\n",
    "\n",
    "# In the following cicle, for each event, we evaluate the correlation of the k nearest neighbors in the original and embedded space.\n",
    "correlation_vector = np.zeros(n_samples) # One value for each event \n",
    "for i in range(n_samples):\n",
    "    # For the ith event, we identify the closest k events in the original space...\n",
    "    closest_orig_indices = np.argsort(dist_orig[i])[1:k+1]\n",
    "    # ... and the closest k in the embedded space\n",
    "    closest_latent_indices = np.argsort(dist_latent[i])[1:k+1]\n",
    "    # We evaluate the distances of these events in the final embedded space\n",
    "    closest_orig_distances = dist_latent[i][closest_orig_indices]\n",
    "    closest_latent_distances = dist_latent[i][closest_latent_indices]\n",
    "    # And we find the Pearson correlation between the distance vectors\n",
    "    correlation, _ = pearsonr(closest_orig_distances, closest_latent_distances)\n",
    "    correlation_vector[i] = correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "114d600a",
   "metadata": {},
   "source": [
    "We have now correlation values: to what we can compare them? We need a null distribution!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79a01fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One distribution is not statistically significant enough, we need to compute it several times.\n",
    "num_permutations = 3\n",
    "# We define our final vector: we will fill it later\n",
    "null_distributions = np.zeros((num_permutations, n_samples))\n",
    "\n",
    "# We loop through number of permutations\n",
    "for perm in range(num_permutations):\n",
    "    \n",
    "    # Permute features: we create a dataset with no meaningfull neighborhood structure\n",
    "    permuted_original = np.zeros_like(dataset)\n",
    "    for j,row in enumerate(dataset):\n",
    "        iii = np.random.permutation(dataset.shape[1])\n",
    "        permuted_original[j] = row[iii]\n",
    "    # We generate the UMAP embedding of the permuted dataset\n",
    "    reducer = umap.UMAP(n_neighbors=NN, min_dist=MD, n_components=2,\n",
    "                        metric='euclidean', n_epochs=NEPOCHS, local_connectivity=LOC_CON,\n",
    "                        learning_rate=LR, random_state=perm)\n",
    "    permuted_latent = reducer.fit_transform(permuted_original)\n",
    "    \n",
    "    # We calculate the same correlation we evaluated for the actual dataset, but ths time for the permuted one\n",
    "    dist_orig = pairwise_distances(permuted_original)\n",
    "    dist_latent = pairwise_distances(permuted_latent)\n",
    "    for i in range(n_samples):\n",
    "        closest_orig_indices = np.argsort(dist_orig[i])[1:k+1]\n",
    "        closest_latent_indices = np.argsort(dist_latent[i])[1:k+1]\n",
    "        null_distributions[perm,i], _ = pearsonr(dist_latent[i][closest_orig_indices],\n",
    "                                                 dist_latent[i][closest_latent_indices])\n",
    "        \n",
    "# We flatten the null distribution: we performed more than one permutation only for statistical reasons.\n",
    "null_distributions = null_distributions.flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f1e9761",
   "metadata": {},
   "source": [
    "Now that we have a correlation vector and a null distribution of correlations, we can assess the reliability of each event, by comparing its correlation value to the null distribution. \n",
    "- **Green** = Trustworthy (above 95th percentile).  \n",
    "- **Red** = Dubious (below 5th percentile).  \n",
    "- **Gray** = Neither (in between).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e13e2f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We evaluate the 95th and 5th percentile of the null distribution.\n",
    "percentile_95 = np.percentile(null_distributions, 95)\n",
    "percentile_5 = np.percentile(null_distributions, 5)\n",
    "\n",
    "# We plot the null and the actual correlation distributions.\n",
    "bins = 150\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.hist(null_distributions, bins=bins, density=True, histtype='step',\n",
    "         color='black', linewidth=2, label=\"Null distribution\")\n",
    "plt.hist(correlation_vector, bins=bins, density=True, histtype='step',\n",
    "         color='blue', linewidth=2, label=\"Real embedding\")\n",
    "plt.axvline(percentile_5, color='red', linestyle='--', label=\"5th percentile\")\n",
    "plt.axvline(percentile_95, color='green', linestyle='--', label=\"95th percentile\")\n",
    "plt.xlabel(\"Pearson Correlation\")\n",
    "plt.ylabel(\"Density\")\n",
    "plt.legend()\n",
    "plt.title(\"Real vs Null Reliability Distributions\")\n",
    "\n",
    "# We define trustworthy and dubious events, as previously described.\n",
    "significativity = (correlation_vector >= percentile_95).astype(int)\n",
    "dub = (correlation_vector <= percentile_5)\n",
    "significativity[dub] = -1\n",
    "\n",
    "\n",
    "# Plot the results!\n",
    "colors = {1: 'green', 0: 'gray', -1: 'red'}\n",
    "labels = {1: 'Trustworthy', 0: 'Unlabeled', -1: 'Dubious'}\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "for val, color in colors.items():\n",
    "    idx = significativity == val\n",
    "    plt.scatter(X_latent[idx,0], X_latent[idx,1], s=20, alpha=0.7, c=color, label=labels[val])\n",
    "plt.xlabel(\"UMAP-1\")\n",
    "plt.ylabel(\"UMAP-2\")\n",
    "plt.title(\"GRB Embedding Reliability\")\n",
    "plt.legend()\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eabc7726",
   "metadata": {},
   "source": [
    "\n",
    "## 📝 Exercises\n",
    "\n",
    "1. Change the UMAP parameters (or change the whole dimensionality reduction technique!) and re-run the notebook.  \n",
    "   - How does the number of dubious points change?  \n",
    "   - How does the overall structure of the embedding change?  \n",
    "\n",
    "3. Reflect on the physical meaning:  \n",
    "   - Why is it important to identify dubious embeddings in GRB latent space?  \n",
    "   - How might misleading embeddings affect astrophysical interpretations?  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bddd30dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
