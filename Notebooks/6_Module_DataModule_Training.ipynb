{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d8b886b",
   "metadata": {},
   "source": [
    "# Module / DataModule / Trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02fc6e39",
   "metadata": {},
   "source": [
    "Inspired by open-source libraries such as PyTorch Lightning 70 , at a high level we wish to have three classes: \n",
    "- (i) Module contains models, losses, and optimization methods; \n",
    "- (ii) DataModule provides data loaders for training and validation; define features\n",
    "- (iii) both classes are combined using the Trainer class, which allows us to train models on a variety of hardware platforms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d59100f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from d2l import torch as d2l"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dcbc663",
   "metadata": {},
   "source": [
    "## Step 1: Define the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca361722",
   "metadata": {},
   "source": [
    "At the very least we need three methods. \n",
    "- The first, __init__, stores the learnable parameters, \n",
    "- the __training_step__ method accepts a data batch to return the loss value, \n",
    "- and finally, __configure_optimizers__ returns the optimization method, or a list of them, that is used to update the learnable parameters. \n",
    "\n",
    "Optionally we can define \n",
    "- __validation_step__ to report the evaluation measures. \n",
    "\n",
    "Sometimes we put the code for computing the output into a separate __forward__ method to\n",
    "make it more reusable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "00cf4fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Module(nn.Module, d2l.HyperParameters):  #@save\n",
    "    # nn.Module: a class in torch; \n",
    "    # You may notice that Module is a subclass of nn.Module, the base class of neural networks in PyTorch.\n",
    "    \"\"\"The base class of models.\"\"\"\n",
<<<<<<< HEAD
    "    def __init__(self, plot_train_per_epoch=2, plot_valid_per_epoch=1):\n",
    "        super().__init__() # invokes the initializer (__init__ method) of a parent (or superclass) when you’re working with class inheritance.\n",
=======
    "    def __init__(self, plot_train_per_epoch=2, plot_valid_per_epoch=1): #save hyperparams\n",
    "        super().__init__()  #invoke initialiers from superclass()\n",
>>>>>>> 2391acc (Notebook #5 and 6)
    "        self.save_hyperparameters()\n",
    "        self.board = d2l.ProgressBoard()\n",
    "\n",
    "    def loss(self, y_hat, y):   # add loss function later\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def forward(self, X):   #passing data to model; evaluate\n",
    "        assert hasattr(self, 'net'), 'Neural network is defined'\n",
    "        return self.net(X)  #passing data X through net() method\n",
    "    \n",
    "    def plot(self, key, value, train):  #progress board\n",
    "        \"\"\"Plot a point in animation.\"\"\"\n",
    "        assert hasattr(self, 'trainer'), 'Trainer is not inited'\n",
    "        self.board.xlabel = 'epoch'\n",
    "        if train:\n",
    "            x = self.trainer.train_batch_idx / \\\n",
    "                self.trainer.num_train_batches\n",
    "            n = self.trainer.num_train_batches / \\\n",
    "                self.plot_train_per_epoch\n",
    "        else:\n",
    "            x = self.trainer.epoch + 1\n",
    "            n = self.trainer.num_val_batches / \\\n",
    "                self.plot_valid_per_epoch\n",
    "        self.board.draw(x, value.to(d2l.cpu()).detach().numpy(),\n",
    "                        ('train_' if train else 'val_') + key,\n",
    "                        every_n=int(n))\n",
    "        \n",
    "    def training_step(self, batch): #access a batch of data and return a loss value\n",
    "        l = self.loss(self(*batch[:-1]), batch[-1])\n",
    "        self.plot('loss', l, train=True)\n",
    "        return l\n",
    "    \n",
    "    def validation_step(self, batch):   #same as training step; instead of updating weight, not have activate set; compute gradient or not\n",
    "        l = self.loss(self(*batch[:-1]), batch[-1])\n",
    "        self.plot('loss', l, train=False)\n",
    "\n",
    "    def configure_optimizers(self): #define which algorithm to update weights and bias (w, b)\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d733880",
   "metadata": {},
   "source": [
    "## Step 2: Set Up the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4971e6ea",
   "metadata": {},
   "source": [
    "- Quite frequently the __init__ method is used to prepare the data. This includes downloading and preprocessing if needed. \n",
    "\n",
    "- The __train_dataloader__ returns the data loader for the training dataset. A data loader is a (Python) generator that yields a data batch each time it is used. This batch is then fed into the __training_step__ method of Module to compute the loss. \n",
    "\n",
    "- There is an optional __val_dataloader__ to return the validation dataset loader. It behaves in the same manner, except that it yields data batches for the validation_step method in Module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6404789b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataModule(d2l.HyperParameters):  #@save  #inherit from Hyperparameters\n",
    "    \"\"\"The base class of data.\"\"\"\n",
    "    def __init__(self, root='../data', num_workers=4):\n",
    "        self.save_hyperparameters()\n",
    "    def get_dataloader(self, train):\n",
    "        raise NotImplementedError\n",
    "    def train_dataloader(self):\n",
    "        return self.get_dataloader(train=True)\n",
    "    def val_dataloader(self):\n",
    "        return self.get_dataloader(train=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c27b83e",
   "metadata": {},
   "source": [
    "## Step 3: Prepare for training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fce0057",
   "metadata": {},
   "source": [
    "The __Trainer__ class trains the learnable parameters in the Module class with data specified in DataModule. \n",
    "\n",
    "- The key method is __fit__, which accepts two arguments: model, an instance of Module, and data, an instance of DataModule. \n",
    "\n",
    "- It then iterates over the entire dataset __max_epochs__ times to train the model. As before, we will defer the implementation of this method to later chapters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "22764608",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer(d2l.HyperParameters):  #@save\n",
    "    \"\"\"The base class for training models with data.\"\"\"\n",
    "    def __init__(self, max_epochs, num_gpus=0, gradient_clip_val=0):\n",
    "        self.save_hyperparameters()\n",
    "        assert num_gpus == 0, 'No GPU support yet'\n",
    "    def prepare_data(self, data):   #load the dataLoader\n",
    "        self.train_dataloader = data.train_dataloader()\n",
    "        self.val_dataloader = data.val_dataloader()\n",
    "        self.num_train_batches = len(self.train_dataloader)\n",
    "        self.num_val_batches = (len(self.val_dataloader)\n",
    "                                if self.val_dataloader is not None else 0)\n",
    "    def prepare_model(self, model): # call the module trainer\n",
    "        model.trainer = self\n",
    "        model.board.xlim = [0, self.max_epochs]\n",
    "        self.model = model\n",
    "    def fit(self, model, data): #\n",
    "        self.prepare_data(data) #prepare data\n",
    "        self.prepare_model(model)   #prepare model\n",
    "        self.optim = model.configure_optimizers()   #optimizer call the class Module (above)\n",
    "        self.epoch = 0\n",
    "        self.train_batch_idx = 0\n",
    "        self.val_batch_idx = 0\n",
    "        for self.epoch in range(self.max_epochs):\n",
    "            self.fit_epoch()\n",
    "    def fit_epoch(self):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4074fe9b",
   "metadata": {},
   "source": [
    "## Synthetic Data creation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "790197fe",
   "metadata": {},
   "source": [
    "In the example in th enext Notebook (Linear regression), we will use a syntthetic dataset, created to illustrate the imlementation on the NN for Liner Regression. Here you can take a look at how this is done:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c8daf42b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e27b83f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SyntheticRegressionData(d2l.DataModule):  #@save\n",
    "    # since it is regression data, it = 1D tensor\n",
    "    \"\"\"Synthetic data for linear regression.\"\"\"\n",
<<<<<<< HEAD
    "    def __init__(self, w, b, noise=0.01, num_train=10000, num_val=1000, batch_size=32):  \n",
=======
    "    def __init__(self, w, b, noise=0.01, num_train=1000, num_val=1000, batch_size=32):  \n",
>>>>>>> 2391acc (Notebook #5 and 6)
    "        #noise = stdev of the Gaussian\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        n = num_train + num_val\n",
    "        self.X = torch.randn(n, len(w))\n",
    "        noise = torch.randn(n, 1) * noise\n",
    "        self.y = torch.matmul(self.X, w.reshape((-1, 1))) + b + noise   \n",
    "        # -1 is a special value in NumPy that means: “automatically calculate this dimension \n",
    "        # based on the array’s total size.”\n"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 39,
=======
   "execution_count": 29,
>>>>>>> 2391acc (Notebook #5 and 6)
   "id": "f0f44809",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
<<<<<<< HEAD
       "<__main__.SyntheticRegressionData at 0x150caf220>"
      ]
     },
     "execution_count": 39,
=======
       "<__main__.SyntheticRegressionData at 0x13206aaf0>"
      ]
     },
     "execution_count": 29,
>>>>>>> 2391acc (Notebook #5 and 6)
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = SyntheticRegressionData(w=torch.tensor([2, -3.4]), b=4.2)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 40,
=======
   "execution_count": 30,
>>>>>>> 2391acc (Notebook #5 and 6)
   "id": "a16441f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "features: tensor([-0.5556,  0.4157]) \n",
      "label: tensor([1.6653])\n"
=======
      "features: tensor([-0.2117, -1.5465]) \n",
      "label: tensor([9.0445])\n"
>>>>>>> 2391acc (Notebook #5 and 6)
     ]
    }
   ],
   "source": [
    "print('features:', data.X[0],'\\nlabel:', data.y[0])\n",
    "print('number of training examples:', data.num_train)\n",
    "print('number of validation examples:', data.num_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d35feb",
   "metadata": {},
   "source": [
    "Now create the trinloader to load the train set in batches to pass to the training"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 41,
=======
   "execution_count": 31,
>>>>>>> 2391acc (Notebook #5 and 6)
   "id": "40bfde12",
   "metadata": {},
   "outputs": [],
   "source": [
    "@d2l.add_to_class(SyntheticRegressionData)\n",
    "def get_dataloader(self, train):    #add get_dataloader to class SyntheticRegression...\n",
    "    if train:\n",
    "        indices = list(range(0, self.num_train))\n",
    "        # The examples are read in random order\n",
    "        random.shuffle(indices)\n",
    "    else:\n",
    "        indices = list(range(self.num_train, self.num_train+self.num_val))\n",
    "    for i in range(0, len(indices), self.batch_size):\n",
    "        batch_indices = torch.tensor(indices[i: i+self.batch_size])\n",
    "        # Pauses Execution and Returns a Value: When a generator function encounters a yield statement, \n",
    "        # it pauses its execution and returns the value specified by yield to the caller.\n",
    "        yield self.X[batch_indices], self.y[batch_indices] \n"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 42,
=======
   "execution_count": 33,
>>>>>>> 2391acc (Notebook #5 and 6)
   "id": "b641ac43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: torch.Size([32, 2]) \n",
<<<<<<< HEAD
      "y shape: torch.Size([32, 1])\n",
      "Total batches: 313\n"
=======
      "y shape: torch.Size([32, 1])\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "object of type 'generator' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#next example, if rerun multple time, show diff thing\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mX shape:\u001b[39m\u001b[38;5;124m'\u001b[39m, X\u001b[38;5;241m.\u001b[39mshape, \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124my shape:\u001b[39m\u001b[38;5;124m'\u001b[39m, y\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m#  NOTE:\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# iter(...) The iter() function is a built-in Python function that \u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# returns an iterator from an iterable. In this case, it turns the \u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# — in this context, it gives you the first batch of data from the \u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# data loader.\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: object of type 'generator' has no len()"
>>>>>>> 2391acc (Notebook #5 and 6)
     ]
    }
   ],
   "source": [
    "X, y = next(iter(data.train_dataloader()))  \n",
    "#next example, if rerun multple time, show diff thing\n",
    "print('X shape:', X.shape, '\\ny shape:', y.shape)\n",
    "\n",
<<<<<<< HEAD
    "# check number of batches\n",
    "num_batches = sum(1 for _ in iter(data.train_dataloader()))\n",
    "print(\"Total batches:\", num_batches)\n",
=======
>>>>>>> 2391acc (Notebook #5 and 6)
    "\n",
    "#  NOTE:\n",
    "# iter(...) The iter() function is a built-in Python function that \n",
    "# returns an iterator from an iterable. In this case, it turns the \n",
    "# data loader (which is iterable) into an explicit iterator. \n",
    "# This allows you to manually retrieve elements using next().\n",
    "\n",
    "# next(...)\n",
    "# The next() function retrieves the next item from the iterator \n",
    "# — in this context, it gives you the first batch of data from the \n",
    "# data loader."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e55014bc",
   "metadata": {},
   "source": [
    "Rather than writing our own iterator, we can call the existing API in a framework to load data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "348c54eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "@d2l.add_to_class(d2l.DataModule)  #@save\n",
    "def get_tensorloader(self, tensors, train, indices=slice(0, None)):\n",
    "    tensors = tuple(a[indices] for a in tensors)\n",
    "    dataset = torch.utils.data.TensorDataset(*tensors)\n",
    "    return torch.utils.data.DataLoader(dataset, self.batch_size,\n",
    "                                      shuffle=train)\n",
    "\n",
    "@d2l.add_to_class(SyntheticRegressionData)  #@save\n",
    "def get_dataloader(self, train):\n",
    "    i = slice(0, self.num_train) if train else slice(self.num_train, None)\n",
    "    return self.get_tensorloader((self.X, self.y), train, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "623ef185",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: torch.Size([32, 2]) \n",
      "y shape: torch.Size([32, 1])\n"
     ]
    }
   ],
   "source": [
    "X, y = next(iter(data.train_dataloader()))\n",
    "print('X shape:', X.shape, '\\ny shape:', y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b46debeb",
   "metadata": {},
   "source": [
    "## Exercizes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf9669f",
   "metadata": {},
   "source": [
    "### Ex 1:\n",
    "\n",
    "Locate full implementations of the above classes that are saved in the D2L library. We strongly recommend that you look at the implementation in detail once you have gained some more familiarity with deep learning modeling.\n",
    "\n",
    "https://github.com/d2l-ai/d2l-en/tree/master/d2l\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10e310e4",
   "metadata": {},
   "source": [
    "I have investigated the d2l library"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e866ea4",
   "metadata": {},
   "source": [
    "### Ex 2: Do the following:\n",
    "\n",
    "- print the shape of the input X and the label y generated in the example in cell 14\n",
    "- print the length for the train_dataloader object: what does this correspond to?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c35a30b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 2]) torch.Size([32, 1])\n"
     ]
    }
   ],
   "source": [
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a26b6d13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "313"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data.train_dataloader())    #correspond to the number of batches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c15ef3",
   "metadata": {},
   "source": [
    "### Ex 3:\n",
    "\n",
    "The code below plots the data set that has been simulated before. Can you overlay the ground truth?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de211732",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.scatter(data.X[:, 0].detach().numpy(), data.y.detach().numpy(),\n",
    "            1, color='C0', alpha=0.5)\n",
    "plt.scatter(data.X[:, 1].detach().numpy(), data.y.detach().numpy(),\n",
    "            1, color='C1', alpha=0.5)\n",
<<<<<<< HEAD
<<<<<<< HEAD
    "# Hint: create and X array and the function y = 2*x + 4.2 and -3.4*x + 4.2 to plot the ground truth\n",
    "# ...\n",
<<<<<<< HEAD
=======
=======
=======
>>>>>>> 80b951a (minor change)
    "# Hint: create and X array and the function y = 2*x + 4.2 and - 3.4*x2 to plot the ground truth\n",
    "X = torch.randn(12)\n",
    "X2 = torch.randn(12)\n",
    "y = 2*X + 4.2\n",
    "# y2 = \n",
    "# ...\n",
>>>>>>> 592b258 (Practice questions)
    "\n",
    "plt.legend()\n",
    "plt.xlabel('x1 / x2')\n",
    "plt.ylabel('y')\n",
    "plt.title('Synthetic Regression Data')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a06ec84",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
